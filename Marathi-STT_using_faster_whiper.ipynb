{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs Marathi Speech to Text with faster_whisper on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faster-whisper sounddevice ctranslate2 torch numpy==1.26.4 transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to ctranslate2 to use with faster_whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ct2-transformers-converter --model Viraj008/whisper-small-mr --output_dir Viraj008/whisper-small-mr-ct2\n",
    "# !ct2-transformers-converter --model Viraj008/whisper-small-mr_v3 --output_dir Viraj008/whisper-small-mr-v3-ct2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import time\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "\n",
    "# Initialize the faster-whisper model\n",
    "model_size =\"Viraj008\\whisper-small-mr-ct2\"\n",
    "# model_size = \"Viraj008/whisper-small-mr-v3-ct2\" \n",
    "\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Transcribe audio using the faster-whisper model\n",
    "        segments, info = model.transcribe(audio_chunk, beam_size=5, without_timestamps=True)\n",
    "        \n",
    "        print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "        # Extract transcription text\n",
    "        transcription = \" \".join(segment.text for segment in segments)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "\n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter ipywidgets pyaudio faster-whisper ctranslate2 torch numpy==1.26.4 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import time\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "# Initialize the faster-whisper model\n",
    "model_size = \"Viraj008/whisper-small-mr-ct2\"\n",
    "# model_size = \"Viraj008/whisper-small-mr-v3-ct2\"\n",
    "\n",
    "model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Transcribe audio using the faster-whisper model\n",
    "        segments, info = model.transcribe(audio_chunk, beam_size=5, without_timestamps=True)\n",
    "        \n",
    "        print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "        # Extract transcription text\n",
    "        transcription = \" \".join(segment.text for segment in segments)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "\n",
    "    # Convert byte string audio data to numpy array\n",
    "    audio_chunk = np.frombuffer(in_data, dtype=np.float32)\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "    return (in_data, pyaudio.paContinue)\n",
    "\n",
    "# Initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    stream = p.open(format=pyaudio.paFloat32,\n",
    "                    channels=1,\n",
    "                    rate=fs,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=int(fs * block_duration),\n",
    "                    stream_callback=audio_callback)\n",
    "\n",
    "    stream.start_stream()\n",
    "\n",
    "    print(\"Recording... Speak into the microphone.\")\n",
    "\n",
    "    # Keep the script running to continuously process audio input\n",
    "    while stream.is_active():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")\n",
    "\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
