{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live Transcription with audio block size using sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "    \n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "\n",
    "    if not np.any(audio_chunk):\n",
    "        print(\"No audio captured in this chunk.\")\n",
    "        return\n",
    "    # Transcribe the audio chunk\n",
    "    transcription = get_transcription(audio_chunk)\n",
    "    # Print the transcription\n",
    "    print(transcription[0], flush=True)\n",
    "    \n",
    "# Parameters for audio stream\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 5  # Block size in seconds\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live Transcription with audio block size using pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import requests\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def audio_callback(in_data, frame_count, time_info, status):\n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = np.frombuffer(in_data, dtype=np.int16).flatten()\n",
    "\n",
    "    if not np.any(audio_chunk):\n",
    "        print(\"No audio captured in this chunk.\")\n",
    "        return (None, pyaudio.paContinue)\n",
    "    \n",
    "    # Transcribe the audio chunk\n",
    "    transcription = get_transcription(audio_chunk)\n",
    "    # Print the transcription\n",
    "    print(transcription[0], flush=True)\n",
    "    \n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Parameters for audio stream\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 5  # Block size in seconds\n",
    "chunk_size = int(fs * block_duration)\n",
    "\n",
    "# Create a PyAudio instance\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "# Open an audio stream\n",
    "try:\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                    channels=1,\n",
    "                    rate=fs,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=chunk_size,\n",
    "                    stream_callback=audio_callback)\n",
    "    \n",
    "    print(\"Recording... Speak into the microphone.\")\n",
    "    stream.start_stream()\n",
    "    \n",
    "    while stream.is_active():\n",
    "        # Keep the script running to continuously process audio input\n",
    "        time.sleep(0.1)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Stop and close the stream\n",
    "    if 'stream' in locals():\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live Transcription with pause ditection using sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak into the microphone.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KITS TTS\\Marathi-Speech-To-Text\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for transcription: 0.73 seconds\n",
      "Transcription: तू को नाही?\n",
      "Time taken for transcription: 0.97 seconds\n",
      "Transcription: तो ते नावक आहे.\n",
      "Time taken for transcription: 0.82 seconds\n",
      "Transcription: तुझा नावकाय.\n",
      "Time taken for transcription: 0.70 seconds\n",
      "Transcription: च्च\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording... Speak into the microphone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;66;03m# Keep the script running to continuously process audio input\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with audio stream: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "    \n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription[0]}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr_v3\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr_v3\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "    \n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription[0]}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Live Transcription with pause ditection using pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import requests\n",
    "import pyaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 4  # Block size in seconds\n",
    "silence_threshold = 50  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def callback(in_data, frame_count, time_info, status):\n",
    "    global last_audio_time\n",
    "    audio_chunk = np.frombuffer(in_data, dtype=np.int16)\n",
    "    \n",
    "    current_time = time.time()\n",
    "\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            if buffer:\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription[0]}\", flush=True)\n",
    "                buffer.clear()\n",
    "    else:\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "    return (None, pyaudio.paContinue)\n",
    "\n",
    "# Create an audio stream using PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=fs,\n",
    "                input=True,\n",
    "                frames_per_buffer=int(fs * block_duration),\n",
    "                stream_callback=callback)\n",
    "\n",
    "try:\n",
    "    print(\"Recording... Speak into the microphone.\")\n",
    "    stream.start_stream()\n",
    "\n",
    "    while stream.is_active():\n",
    "        time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")\n",
    "finally:\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording... Speak into the microphone.\n",
      "Time taken for transcription: 0.90 seconds\n",
      "Transcription: तेचे नाव काय आहे?\n",
      "Time taken for transcription: 0.80 seconds\n",
      "Transcription: तू काय कधतोस?\n",
      "Time taken for transcription: 0.97 seconds\n",
      "Transcription: तुला कोणी बनवलेले आहे.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording... Speak into the microphone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;66;03m# Keep the script running to continuously process audio input\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with audio stream: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr_v3\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr_v3\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "    \n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription[0]}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
