{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import requests\n",
    "import sounddevice as sd\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "\n",
    "# Function to detect the operating system and decide on GPU support\n",
    "def get_device():\n",
    "    current_os = platform.system()\n",
    "    print(f\"Detected operating system: {current_os}\")\n",
    "\n",
    "    if current_os == \"Darwin\":  # macOS\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"Using Metal Performance Shaders (MPS) for GPU.\")\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"MPS is not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "    elif current_os == \"Windows\" or current_os == \"Linux\":\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Using CUDA for GPU.\")\n",
    "            return torch.device(\"cuda\")\n",
    "        else:\n",
    "            print(\"CUDA is not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "    else:\n",
    "        print(\"Unsupported operating system. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Get the appropriate device based on the OS\n",
    "device = get_device()\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "model.to(device)\n",
    "\n",
    "# Parameters\n",
    "fs = 16000  # Sample rate\n",
    "block_duration = 2  # Block size in seconds\n",
    "silence_threshold = 0.005  # Energy threshold for detecting silence\n",
    "pause_duration = 1.5  # Minimum pause duration in seconds\n",
    "\n",
    "# Buffer to store audio chunks\n",
    "buffer = []\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def get_transcription(audio_chunk):\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Process audio using the Whisper processor\n",
    "        input_features = processor(audio_chunk, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "        # Generate transcription using the Whisper model\n",
    "        predicted_ids = model.generate(input_features)\n",
    "        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Calculate time taken\n",
    "        time_taken = time.time() - start_time\n",
    "        print(f\"Time taken for transcription: {time_taken:.2f} seconds\")\n",
    "\n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return [\"\"]\n",
    "\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    global last_audio_time\n",
    "    if status:\n",
    "        print(f\"Status: {status}\", flush=True)\n",
    "\n",
    "    # Convert audio chunk to numpy array and flatten to 1D\n",
    "    audio_chunk = indata[:, 0].flatten()\n",
    "    current_time = time.time()\n",
    "\n",
    "    # Check for silence\n",
    "    if np.mean(np.abs(audio_chunk)) < silence_threshold:\n",
    "        if current_time - last_audio_time > pause_duration:\n",
    "            # Accumulate audio data\n",
    "            if buffer:\n",
    "                # Process accumulated audio data\n",
    "                combined_audio = np.concatenate(buffer)\n",
    "                transcription = get_transcription(combined_audio)\n",
    "                print(f\"Transcription: {transcription[0]}\", flush=True)\n",
    "                buffer.clear()  # Clear the buffer after processing\n",
    "    else:\n",
    "        # Update last audio time and add to buffer\n",
    "        last_audio_time = current_time\n",
    "        buffer.append(audio_chunk)\n",
    "\n",
    "# Create an audio stream\n",
    "try:\n",
    "    with sd.InputStream(callback=audio_callback, channels=1, samplerate=fs, blocksize=int(fs * block_duration)):\n",
    "        print(\"Recording... Speak into the microphone.\")\n",
    "        while True:\n",
    "            # Keep the script running to continuously process audio input\n",
    "            time.sleep(0.1)\n",
    "except Exception as e:\n",
    "    print(f\"Error with audio stream: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected operating system: Windows\n",
      "Using CUDA for GPU.\n",
      "Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampling audio from 24000 Hz to 16000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KITS TTS\\Marathi-Speech-To-Text\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig\n",
    "import gradio as gr\n",
    "from scipy.signal import resample\n",
    "import time\n",
    "\n",
    "# Function to detect the operating system and decide on GPU support\n",
    "def get_device():\n",
    "    current_os = platform.system()\n",
    "    print(f\"Detected operating system: {current_os}\")\n",
    "\n",
    "    if current_os == \"Darwin\":  # macOS\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"Using Metal Performance Shaders (MPS) for GPU.\")\n",
    "            return torch.device(\"mps\")\n",
    "        else:\n",
    "            print(\"MPS is not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "    elif current_os == \"Windows\" or current_os == \"Linux\":\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Using CUDA for GPU.\")\n",
    "            return torch.device(\"cuda\")\n",
    "        else:\n",
    "            print(\"CUDA is not available. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "    else:\n",
    "        print(\"Unsupported operating system. Falling back to CPU.\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Get the appropriate device based on the OS\n",
    "device = get_device()\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = WhisperProcessor.from_pretrained(\"Viraj008/whisper-small-mr\")\n",
    "config_url = \"https://huggingface.co/Viraj008/whisper-small-mr/resolve/main/config.json\"\n",
    "config_response = requests.get(config_url)\n",
    "\n",
    "if config_response.status_code == 200:\n",
    "    config_dict = config_response.json()\n",
    "    config = WhisperConfig.from_dict(config_dict)\n",
    "else:\n",
    "    raise ValueError(\"Failed to load configuration from the specified URL.\")\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"Viraj008/whisper-small-mr\", config=config)\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"mr\", task=\"transcribe\")\n",
    "model.to(device)\n",
    "\n",
    "def transcribe_audio(audio):\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load audio as numpy array\n",
    "    sampling_rate, audio_data = audio\n",
    "\n",
    "    # Check if the sampling rate is not 16000 Hz\n",
    "    if sampling_rate != 16000:\n",
    "        print(f\"Resampling audio from {sampling_rate} Hz to 16000 Hz\")\n",
    "        number_of_samples = round(len(audio_data) * float(16000) / sampling_rate)\n",
    "        audio_data = resample(audio_data, number_of_samples)\n",
    "\n",
    "    # Normalize audio data\n",
    "    audio_data = (audio_data / np.max(np.abs(audio_data))).astype(np.float32)\n",
    "\n",
    "    # Process audio using the Whisper processor\n",
    "    input_features = processor(audio_data, sampling_rate=16000, return_tensors=\"pt\").input_features.to(device)\n",
    "    \n",
    "    # Generate transcription using the Whisper model\n",
    "    predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Calculate transcription time\n",
    "    time_taken = time.time() - start_time\n",
    "    transcription_result = f\"Transcription: {transcription[0]}\\nTime taken: {time_taken:.2f} seconds\"\n",
    "    \n",
    "    return transcription_result\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Whisper Transcription\")\n",
    "    \n",
    "    # Add audio examples\n",
    "    audio_examples = [\n",
    "        \"marathi-audio.wav\"\n",
    "    ]\n",
    "    \n",
    "    audio_input = gr.Audio(label=\"Record or Upload Audio\", type=\"numpy\")\n",
    "    transcription_output = gr.Textbox(label=\"Transcription\")\n",
    "    \n",
    "    # Add radio buttons for audio examples\n",
    "    audio_example_selector = gr.Radio(label=\"Select Audio Example\", choices=audio_examples, type=\"value\")\n",
    "    \n",
    "    # Function to load selected audio example\n",
    "    def load_example(selected_example):\n",
    "        return selected_example\n",
    "    \n",
    "    audio_example_selector.change(load_example, inputs=audio_example_selector, outputs=audio_input)\n",
    "    \n",
    "    audio_input.change(transcribe_audio, inputs=audio_input, outputs=transcription_output)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
